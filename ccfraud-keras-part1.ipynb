{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"border: none\" align=\"left\">\n",
    "   <tr style=\"border: none\">\n",
    "      <th style=\"border: none\"><font face=\"verdana\" size=\"5\" color=\"black\"><b>Use Keras to predict credit card fraud</b></th>\n",
    "      <th style=\"border: none\"><img src=\"https://github.com/pmservice/customer-satisfaction-prediction/blob/master/app/static/images/ml_icon_gray.png?raw=true\" alt=\"Watson Machine Learning icon\" height=\"40\" width=\"40\"></th>\n",
    "   </tr>\n",
    "   <tr style=\"border: none\">\n",
    "       <th style=\"border: none\"><img src=\"http://www.thebluediamondgallery.com/typewriter/images/credit-card-fraud.jpg\" width=\"600\" alt=\"Icon\"/>        </th>\n",
    "   </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the steps and code to create a model that detects credit card fraud. You will use an anonymized data set of credit card transactions,  which is available at Kaggle. The sample data set consists of 285K credit card transactions each with 31 columns. Most of the features of the  data set are the result of PCA except for the Time and Amount. The Time column represente the elapsed time in seconds relative to the  first row in the dataset and the  Amount column represents the dollar amount of each transaction. \n",
    "\n",
    "The data is highly unbalanced as 98.828% are normal transactions and only 0.172% are fraudulent. This means that the overall accuracy of any model you build will not be a good measure of the model's performance. If you guess that every transaction is normal 100% of the time you would be right 99.828% of the time ! \n",
    "\n",
    "You will buld a Logistic Regression model in Keras, which may seem unlikely at first,  as Keras is a typically used for Deep Learning networks,  however Logistic Regression is just a single layer neural network with no hidden layers, so Keras makes this very easy to build. \n",
    "\n",
    "## Learning goals\n",
    "\n",
    "In this notebook, you will learn how to:\n",
    "\n",
    "-  Load a sample data set from ``Kaggle``.\n",
    "-  Explore data.\n",
    "-  Prepare data for training and evaluation.\n",
    "-  Create a Keras model.\n",
    "-  Train and evaluate the  model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"setup\"></a>\n",
    "## 1. Set up the environment\n",
    "\n",
    "Before you use the sample code in this notebook, you must perform the following setup tasks:\n",
    "\n",
    "- Download the credit card data from [here](https://www.kaggle.com/mlg-ulb/creditcardfraud). The name of the file is *creditcard.csv*\n",
    "- Click on the data icon at the top right of the notebook window and then select and upload the *creditcard.csv* file.\n",
    "![Data icon](https://github.com/djccarew/ccfraud-keras-lab-part1/raw/master/images/ss5.png)\n",
    "- Once the file is uploaded, place your cursor in the code cell below and select **Insert to code->Insert pandas Dataframe**\n",
    "![Insert code](https://github.com/djccarew/ccfraud-keras-lab-part1/raw/master/images/ss6.png) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With your cursor in this cell, insert the code to read the dataset into a \n",
    "# DataFrame as instructed above\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"load\"></a>\n",
    "## 2. Load and explore data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section you will load the data from scikit-learn sample data sets and then perform a basic exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New copy of the imported Dataframe\n",
    "# Make sure variable name on the right of this assigment statement matches the value inserted\n",
    "# into the code cell above\n",
    "data =  df_data_1.copy()\n",
    "# The  Time column does  not correlate with either fradulent or normal transactions so we'll drop it \n",
    "data = data.drop(['Time'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, count the data examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of samples.\n",
    "print(\"Number of normal transactions: \", data[data.Class == 0].shape[0])\n",
    "print(\"Number of fraudulent transactions: \", data[data.Class == 1].shape[0])\n",
    "print(\"Total transactions: \" + str(len(data.index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"model\"></a>\n",
    "## 3. Create a Keras model\n",
    "\n",
    "In this section you learn how to:\n",
    "- [Prepare the data](#prep)\n",
    "- [Create the Keras machine learning model](#pipe)\n",
    "- [Train the model](#train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Prepare the data<a id=\"prep\"></a>\n",
    "\n",
    "Standardize the features by removing the mean and scaling to unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "data_standardized = data.copy()\n",
    "for col_name in data_standardized.columns.values:\n",
    "    if col_name != 'Class':\n",
    "        data_standardized[col_name] = StandardScaler().fit_transform(data_standardized[col_name].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this subsection you will split your data into: \n",
    "- Training data set\n",
    "- Validation data set\n",
    "- Testing data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into data sets and display the number of records for each data set.\n",
    "from sklearn.model_selection import train_test_split \n",
    "RANDOM_SEED = 92\n",
    "\n",
    "# First split is 60/40\n",
    "train_data, val_data = train_test_split(data_standardized, test_size=0.4, random_state=RANDOM_SEED)\n",
    "#train_data, test_data = train_test_split(data_standardized, test_size=0.3, random_state=RANDOM_SEED)\n",
    "train_target = train_data['Class']\n",
    "train_data = train_data.drop(['Class'], axis=1)\n",
    "# 2nd split is 50/50\n",
    "val_data, test_data = train_test_split(val_data, test_size=0.5, random_state=RANDOM_SEED)\n",
    "val_target = val_data['Class']\n",
    "val_data =  val_data.drop(['Class'], axis=1)\n",
    "test_target = test_data['Class']\n",
    "test_data = test_data.drop(['Class'], axis=1)\n",
    "\n",
    "print(\"Number of training records: \" + str(len(train_target)))\n",
    "print(\"Number of validation records: \" + str(len(val_target)))\n",
    "print(\"Number of testing records : \" + str(len(test_target)))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Your data has been successfully split into three  data sets: \n",
    "\n",
    "-  The train data set, which is the largest group, is used for training.\n",
    "-  The validation data will be used to evaluate the model as it is being trained.\n",
    "-  The test data set will be used evaluate the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Create the Keras machine learning model<a id=\"model\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section you will create a Keras machine learning model and then train it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import the related Python  packages that are needed in the subsequent steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Keras and related packages\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build and compile  the model. It's a single Dense layer with no hidden layers, softmax activation and Adam optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(2,  # output dim is 2, one per each class\n",
    "                activation='softmax',\n",
    "                input_dim=train_data.shape[1]))  # input dimension = number of features your data has\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Train the model<a id=\"train\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can use the model with the training and validation data to mimimize the loss during training. Since the data is so unbalanced we give give more weight to the few fradulent transactions in the data set (we assign weights  that are the inverse of the relative frequency of fraudulent and normal transactions). This should take a  couple minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "class_weights = {0: 1., 1: len(train_target[train_target == 0])/len(train_target[train_target == 1])}\n",
    "history = model.fit(train_data.values, to_categorical(train_target.values), class_weight=class_weights, epochs=20, batch_size=256, validation_data=(val_data.values, to_categorical(val_target.values))).history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the validation loss by epoch to make sure that the model is converging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(history['loss'])\n",
    "plt.plot(history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use **test data** to generate an evaluation report to check your **model quality**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "# Evaluate your model.\n",
    "predicted = model.predict_classes(test_data.values)\n",
    "\n",
    "print(\"Evaluation report: \\n\\n%s\" % metrics.classification_report(test_target.values, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Note:** Your model is  predicting about 95% of the fraudulent transactions correctly and 98% of the normal ones. The **recall** of the fraudulent and normal transactions is probably the most important measure to keep an eye on (vs overall accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Examine the results <a id=\"evaluate\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a rudimentary cost model so we can see whether or not the model has saved us money. But first  we need to see the avg cost of both fraudulent and normal transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_normal_transaction_amount = data[data.Class == 0][\"Amount\"].mean()\n",
    "mean_fraudulent_transaction_amount = data[data.Class == 1][\"Amount\"].mean()\n",
    "print('Average normal transaction amount %f' % mean_normal_transaction_amount)\n",
    "print('Average fraudulent transaction amount %f' % mean_fraudulent_transaction_amount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we assume that we earn a trasnsaction fee of 2% for each valid transaction then a rudimentary cost model will works as follows:\n",
    "- Every fraudulent transaction detected will save us approx $122.00\n",
    "\n",
    "- Every fraudulent transaction missed  will cost us approx $122.00\n",
    "\n",
    "- Every normal transaction classified as normal will earn  2% of $88\n",
    "\n",
    "- Every normal transaction classified as fraudulent  will cost us 2% of $88  \n",
    "\n",
    "Lets look at a confusion matrix to more insight into the results of the model. The top right quadrant has the  fraudulent transactions recognized by the model as normal and the bottom left has the normal transactions recognized as fraudulent. The bottom right and top left numbers represent the correctly  identified fraudulent and normal transactions respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "LABELS = [\"Normal\", \"Fraudulent\"]\n",
    "conf_matrix = confusion_matrix(predicted, test_target)\n",
    "plt.figure(figsize=(8, 8))\n",
    "heatmap= sns.heatmap(conf_matrix, cmap=\"Blues\",xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt=\"d\");\n",
    "heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n",
    "heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right')\n",
    "plt.title(\"Confusion matrix\")\n",
    "plt.ylabel('True class')\n",
    "plt.xlabel('Predicted class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this you would earn about ```$1.53```  per transaction without the model and ```$1.83``` per transaction with it  so  the model results in  a net positive effect on the bottom line. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 6. Summary and next steps <a id=\"summary\"></a>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "You successfully completed this notebook! \n",
    " \n",
    "You learned how to use Keras to build a Logistic Regression model to predict credit card fraud. \n",
    "\n",
    "Check out our [Online Documentation](https://dataplatform.ibm.com/docs/content/analyze-data/wml-setup.html) for more samples, tutorials, documentation, how-tos, and blog posts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Author\n",
    "\n",
    "**David Carew** is a Developer Advocate at IBM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright © 2017, 2018 IBM. This notebook and its source code are released under the terms of the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#F5F7FA; height:110px; padding: 2em; font-size:14px;\">\n",
    "<span style=\"font-size:18px;color:#152935;\">Love this notebook? </span>\n",
    "<span style=\"font-size:15px;color:#152935;float:right;margin-right:40px;\">Don't have an account yet?</span><br>\n",
    "<span style=\"color:#5A6872;\">Share it with your colleagues and help them discover the power of Watson Studio!</span>\n",
    "<span style=\"border: 1px solid #3d70b2;padding:8px;float:right;margin-right:40px; color:#3d70b2;\"><a href=\"https://ibm.co/wsnotebooks\" target=\"_blank\" style=\"color: #3d70b2;text-decoration: none;\">Sign Up</a></span><br>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
